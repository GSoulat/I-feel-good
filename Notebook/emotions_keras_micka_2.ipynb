{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import texthero as hero\n",
    "# tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../emotion_diagramme.svg' alt='emotions' style='background-color: white' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_used = 'kaggle clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\data\\kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micka\\AppData\\Local\\Temp\\ipykernel_34396\\3146230756.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['Text'] = df['Text'].str.replace(p,'')\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage\n",
    "\n",
    "import string\n",
    "punctuation = string.punctuation\n",
    "for p in punctuation:\n",
    "    df['Text'] = df['Text'].str.replace(p,'')\n",
    "\n",
    "df['Text'] = df['Text'].replace('(http://|https://|ftp://|ssh://)\\S*','',regex=True)\n",
    "df['Text'] = df['Text'].replace('@\\S*','',regex=True)\n",
    "\n",
    "stops_r = list(map(lambda x: x.replace(\"'\",\"\"), stopwords.words('english')))\n",
    "stops = set(stopwords.words('english') + stops_r)\n",
    "for word in stops:\n",
    "    df['Text'] = df['Text'].str.replace(' ' + word + ' ', ' ')\n",
    "df['Text'] = df['Text'].str.replace('^i ', '', regex=True).replace('^im ', '', regex=True).replace('^I ', '', regex=True).replace('^Im ', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17167,)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "oov_token = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17165"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))#max_length\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_avg = []\n",
    "for i in range(len(X_train_sequences)):\n",
    "    avg = np.zeros(100)\n",
    "    for w in X_train_sequences[i]:\n",
    "        avg += embedding_matrix[w]\n",
    "    avg = avg/len(X_train_sequences[i])\n",
    "    X_train_avg.append(avg)\n",
    "X_train_avg = np.asarray(X_train_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_avg = []\n",
    "for i in range(len(X_test_sequences)):\n",
    "    avg = np.zeros(100)\n",
    "    for w in X_test_sequences[i]:\n",
    "        avg += embedding_matrix[w]\n",
    "    avg = avg/len(X_test_sequences[i])\n",
    "    X_test_avg.append(avg)\n",
    "X_test_avg = np.asarray(X_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_activation = 'relu'\n",
    "output_activation = 'softmax'\n",
    "# model_description = 'adding_glove_embedding_vectors ; Dense:128:{} ; Dense:64:{} ;Dense:6:{}'.format(layer_activation,output_activation)\n",
    "\n",
    "input_dim = len(X_train_avg[0])\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=input_dim, activation=layer_activation),\n",
    "    Dense(128, input_dim=input_dim, activation=layer_activation),\n",
    "    Dense(6, activation=output_activation)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {'happy':0, 'sadness':1, 'anger':2, 'fear':3, 'love':4, 'surprise':5}\n",
    "y_train = y_train.replace(emotions)\n",
    "y_test = y_test.replace(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 256)               25856     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125,318\n",
      "Trainable params: 125,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "# import tensorflow_addons as tfa \n",
    "# f1 = tfa.metrics.F1Score(6)\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metric = metrics.SparseCategoricalAccuracy()\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[metrics.SparseCategoricalAccuracy()])#metrics.AUC(multi_label=True, num_classes=6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 1.2262 - sparse_categorical_accuracy: 0.5265 - val_loss: 1.0760 - val_sparse_categorical_accuracy: 0.5969\n",
      "Epoch 2/200\n",
      "2146/2146 [==============================] - 4s 2ms/step - loss: 1.0972 - sparse_categorical_accuracy: 0.5799 - val_loss: 1.0485 - val_sparse_categorical_accuracy: 0.5972\n",
      "Epoch 3/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 1.0366 - sparse_categorical_accuracy: 0.6016 - val_loss: 1.0181 - val_sparse_categorical_accuracy: 0.6109\n",
      "Epoch 4/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.9803 - sparse_categorical_accuracy: 0.6208 - val_loss: 1.0761 - val_sparse_categorical_accuracy: 0.5937\n",
      "Epoch 5/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.9328 - sparse_categorical_accuracy: 0.6409 - val_loss: 1.0017 - val_sparse_categorical_accuracy: 0.6244\n",
      "Epoch 6/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.8810 - sparse_categorical_accuracy: 0.6648 - val_loss: 0.9706 - val_sparse_categorical_accuracy: 0.6230\n",
      "Epoch 7/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.8300 - sparse_categorical_accuracy: 0.6795 - val_loss: 1.0112 - val_sparse_categorical_accuracy: 0.6286\n",
      "Epoch 8/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.7807 - sparse_categorical_accuracy: 0.6974 - val_loss: 1.0350 - val_sparse_categorical_accuracy: 0.6156\n",
      "Epoch 9/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.7311 - sparse_categorical_accuracy: 0.7150 - val_loss: 1.0814 - val_sparse_categorical_accuracy: 0.6274\n",
      "Epoch 10/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.6857 - sparse_categorical_accuracy: 0.7389 - val_loss: 1.0867 - val_sparse_categorical_accuracy: 0.6240\n",
      "Epoch 11/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.6396 - sparse_categorical_accuracy: 0.7534 - val_loss: 1.1338 - val_sparse_categorical_accuracy: 0.6118\n",
      "Epoch 12/200\n",
      "2146/2146 [==============================] - 5s 3ms/step - loss: 0.5955 - sparse_categorical_accuracy: 0.7679 - val_loss: 1.1186 - val_sparse_categorical_accuracy: 0.6351\n",
      "Epoch 13/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.5472 - sparse_categorical_accuracy: 0.7880 - val_loss: 1.1886 - val_sparse_categorical_accuracy: 0.6233\n",
      "Epoch 14/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.5114 - sparse_categorical_accuracy: 0.7984 - val_loss: 1.2602 - val_sparse_categorical_accuracy: 0.6214\n",
      "Epoch 15/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.4704 - sparse_categorical_accuracy: 0.8172 - val_loss: 1.3981 - val_sparse_categorical_accuracy: 0.6116\n",
      "Epoch 16/200\n",
      "2146/2146 [==============================] - 5s 2ms/step - loss: 0.4428 - sparse_categorical_accuracy: 0.8302 - val_loss: 1.3531 - val_sparse_categorical_accuracy: 0.6226\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "# log_folder = 'logs'\n",
    "callbacks = [\n",
    "            EarlyStopping(patience = 10)\n",
    "            # TensorBoard(log_dir=log_folder)\n",
    "            ]\n",
    "num_epochs = 200\n",
    "history = model.fit(X_train_avg, y_train, epochs=num_epochs, batch_size=8, validation_data=(X_test_avg, y_test),callbacks=callbacks)#,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_load = load_model('../models/neural_lstm_kaggle_clean.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros(100,dtype=np.int64)\n",
    "test[0] = word_index['god']\n",
    "test[1] = word_index['is']\n",
    "test[2] = word_index['love']\n",
    "# test = list(test)\n",
    "test = np.array([test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2319767 , 0.2617607 , 0.31832987, 0.10729696, 0.01612998,\n",
       "        0.06450569]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\micka\\Documents\\Espace_perso\\Études\\Formation\\Machine_learning\\NLP_emotions\\I-feel-good\\notebook\\emotions_keras_micka_2.ipynb Cell 22'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micka/Documents/Espace_perso/%C3%89tudes/Formation/Machine_learning/NLP_emotions/I-feel-good/notebook/emotions_keras_micka_2.ipynb#ch0000048?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micka/Documents/Espace_perso/%C3%89tudes/Formation/Machine_learning/NLP_emotions/I-feel-good/notebook/emotions_keras_micka_2.ipynb#ch0000048?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../models/neural_lstm.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/micka/Documents/Espace_perso/%C3%89tudes/Formation/Machine_learning/NLP_emotions/I-feel-good/notebook/emotions_keras_micka_2.ipynb#ch0000048?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mload(file)\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../models/neural_lstm_kaggle_clean.pkl', 'r') as file:\n",
    "    pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9998891e-01 1.1125703e-05 1.9218303e-09 7.9951297e-12 1.2341030e-08\n",
      "  5.5463226e-15]]\n",
      "{'happy': 0, 'sadness': 1, 'anger': 2, 'fear': 3, 'love': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "test = embeddings_index['i']+embeddings_index['feel']+embeddings_index['good']\n",
    "test = test.reshape(1,100)\n",
    "print(model.predict(test))\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "field_names = ['dataset', 'model_name', 'model_description', 'encoding', 'loss', 'optimizer',\n",
    "            'metric', 'layer_activation', 'output_activation', 'epoch', 'loss_train',\n",
    "            'accuracy_train', 'loss_val', 'accuracy_val']\n",
    "data = [{'dataset':dataset_used,\n",
    "            'model_name':type(model).__name__,\n",
    "            'model_description':model_description,\n",
    "            'encoding':type(tokenizer).__name__,\n",
    "            'loss':loss,\n",
    "            'optimizer':optimizer,\n",
    "            'metric':type(metric).__name__,\n",
    "            'layer_activation':layer_activation,\n",
    "            'output_activation':output_activation,\n",
    "            'epoch':history.epoch[-1],\n",
    "            'loss_train':history.history[loss][-1],\n",
    "            'accuracy_train':history.history['sparse_categorical_accuracy'][-1],\n",
    "            'loss_val':history.history['val_loss'][-1],\n",
    "            'accuracy_val':history.history['val_sparse_categorical_accuracy'][-1],\n",
    "           }]\n",
    "with open('../results/results.csv', 'a') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
    "    # writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4292, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# On crée une nouvelle expérimentation\n",
    "# experiment_id = mlflow.create_experiment(\"NLP_projet\")\n",
    "\n",
    "with mlflow.start_run(experiment_id = 1):\n",
    "\n",
    "    tags = {'dataset':dataset_used,\n",
    "            'model_name':type(model).__name__,\n",
    "            'model_description':model_description,\n",
    "            'encoding':type(tokenizer).__name__,\n",
    "            'loss':loss,\n",
    "            'optimizer':optimizer,\n",
    "            'metric':metric,\n",
    "            'layer_activation':layer_activation,\n",
    "            'output_activation':output_activation,\n",
    "            'epoch':history.epoch[-1],\n",
    "            'loss_train':history.history[loss][-1],\n",
    "            'accuracy_train':history.history['sparse_categorical_accuracy'][-1],\n",
    "            'loss_val':history.history['val_loss'][-1],\n",
    "            'accuracy_val':history.history['val_sparse_categorical_accuracy'][-1],\n",
    "           }\n",
    "    # tags.update({'words_rooting':words_rooting})\n",
    "    mlflow.set_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd34b0786104e267c65f528446d3644880f99c0ab5e1827b2c13f949719e4165"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
